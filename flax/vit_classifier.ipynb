{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "from datasets import load_dataset\n",
    "from jaxtyping import Array, Float, Int, jaxtyped\n",
    "from beartype import beartype\n",
    "from tqdm import tqdm\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nnx.jit = lambda fn: fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lovely_jax\n",
    "\n",
    "lovely_jax.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typed(fn):\n",
    "    return jaxtyped(fn, typechecker=beartype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VITConfig:\n",
    "    in_feature_shape = (32, 32, 3)\n",
    "    out_features = 10\n",
    "    patch_size = 4\n",
    "    num_layers = 8\n",
    "    num_heads = 8\n",
    "    embed_dim = 256\n",
    "    rngs: nnx.Rngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nnx.Module):\n",
    "    def __init__(self, module: nnx.Module):\n",
    "        self.norm = nnx.LayerNorm(\n",
    "            num_features=config.embed_dim,\n",
    "            rngs=config.rngs,\n",
    "        )\n",
    "        self.module = module\n",
    "\n",
    "    @typed\n",
    "    @nnx.jit\n",
    "    def __call__(self, x: Float[Array, \"batch ...\"]) -> Float[Array, \"batch ...\"]:\n",
    "        x = self.norm(x)\n",
    "        return x + self.module(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchify(nnx.Module):\n",
    "    def __init__(self, *, config: VITConfig):\n",
    "        self.config = config\n",
    "        self.conv = nnx.Conv(\n",
    "            in_features=config.in_feature_shape[2],\n",
    "            out_features=config.embed_dim,\n",
    "            kernel_size=(config.patch_size, config.patch_size),\n",
    "            strides=(config.patch_size, config.patch_size),\n",
    "            rngs=config.rngs,\n",
    "        )\n",
    "\n",
    "    @typed\n",
    "    @nnx.jit\n",
    "    def __call__(\n",
    "        self, x: Float[Array, \"batch h w ch\"]\n",
    "    ) -> Float[Array, \"batch patches emb\"]:\n",
    "        x = self.conv(x)\n",
    "        x = x.reshape(x.shape[0], -1, self.config.embed_dim)\n",
    "        cls_token = jax.nn.initializers.truncated_normal(stddev=0.02)(\n",
    "            jax.random.key(0),\n",
    "            dtype=jnp.float32,\n",
    "            shape=(x.shape[0], 1, self.config.embed_dim),\n",
    "        )\n",
    "        x = jnp.concatenate([cls_token, x], axis=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typed\n",
    "@nnx.jit\n",
    "def apply_rope(\n",
    "    q: Float[Array, \"batch n d\"],\n",
    "    k: Float[Array, \"batch n d\"],\n",
    ") -> tuple[Float[Array, \"batch n d\"], Float[Array, \"batch n d\"]]:\n",
    "    return q, k  # TODO: implement rope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnBlock(nnx.Module):\n",
    "    def __init__(self, *, config: VITConfig):\n",
    "        self.config = config\n",
    "        self.qkv = nnx.Linear(\n",
    "            in_features=config.embed_dim,\n",
    "            out_features=config.embed_dim * 3,\n",
    "            rngs=config.rngs,\n",
    "        )\n",
    "\n",
    "    @typed\n",
    "    @nnx.jit\n",
    "    def __call__(\n",
    "        self, x: Float[Array, \"batch patches emb\"]\n",
    "    ) -> Float[Array, \"batch patches emb\"]:\n",
    "        q, k, v = self.qkv(x).split(3, axis=-1)\n",
    "        q, k = apply_rope(q, k)\n",
    "        a = nnx.dot_product_attention(q, k, v)\n",
    "        a = a.reshape(a.shape[0], -1, self.config.embed_dim)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nnx.Module):\n",
    "    def __init__(self, *, config: VITConfig):\n",
    "        self.config = config\n",
    "        self.norm = nnx.LayerNorm(\n",
    "            num_features=config.embed_dim,\n",
    "            rngs=config.rngs,\n",
    "        )\n",
    "        self.linear1 = nnx.Linear(\n",
    "            in_features=config.embed_dim,\n",
    "            out_features=config.embed_dim * 4,\n",
    "            rngs=config.rngs,\n",
    "        )\n",
    "        self.linear2 = nnx.Linear(\n",
    "            in_features=config.embed_dim * 4,\n",
    "            out_features=config.embed_dim,\n",
    "            rngs=config.rngs,\n",
    "        )\n",
    "\n",
    "    @typed\n",
    "    @nnx.jit\n",
    "    def __call__(\n",
    "        self, x: Float[Array, \"batch patches emb\"]\n",
    "    ) -> Float[Array, \"batch patches emb\"]:\n",
    "        x = self.norm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nnx.Module):\n",
    "    def __init__(self, *, config: VITConfig):\n",
    "        self.config = config\n",
    "        self.mha = Residual(AttnBlock(config=config))\n",
    "        self.mlp = Residual(MLP(config=config))\n",
    "\n",
    "    @typed\n",
    "    @nnx.jit\n",
    "    def __call__(\n",
    "        self, x: Float[Array, \"batch patches emb\"]\n",
    "    ) -> Float[Array, \"batch patches emb\"]:\n",
    "        x = self.mha(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nnx.Module):\n",
    "    def __init__(self, *, config: VITConfig):\n",
    "        self.config = config\n",
    "        self.layers = nnx.Sequential(\n",
    "            *[EncoderBlock(config=config) for _ in range(config.num_layers)]\n",
    "        )\n",
    "\n",
    "    @typed\n",
    "    @nnx.jit\n",
    "    def __call__(\n",
    "        self, x: Float[Array, \"batch patches emb\"]\n",
    "    ) -> Float[Array, \"batch patches emb\"]:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nnx.Module):\n",
    "    def __init__(self, *, config: VITConfig):\n",
    "        self.config = config\n",
    "        self.patchify = Patchify(config=config)\n",
    "        self.encoder = Encoder(config=config)\n",
    "\n",
    "    @typed\n",
    "    @nnx.jit\n",
    "    def __call__(\n",
    "        self, x: Float[Array, \"batch h w ch\"]\n",
    "    ) -> Float[Array, \"batch patches emb\"]:\n",
    "        x = self.patchify(x)\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VITClassifier(nnx.Module):\n",
    "    def __init__(self, *, config: VITConfig, num_classes: int):\n",
    "        self.config = config\n",
    "        self.vit = VIT(config=config)\n",
    "        self.linear_probe = nnx.Linear(\n",
    "            in_features=config.embed_dim,\n",
    "            out_features=num_classes,\n",
    "            rngs=config.rngs,\n",
    "        )\n",
    "\n",
    "    @typed\n",
    "    @nnx.jit\n",
    "    def __call__(self, x: Float[Array, \"batch h w ch\"]) -> Float[Array, \"batch c\"]:\n",
    "        x = self.vit(x)  # [batch, patches, emb]\n",
    "        x = x[:, 0, :]  # [batch, emb]\n",
    "        x = self.linear_probe(x)  # [batch, num_classes]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(\n",
    "    X: Float[Array, \"n h w ch\"], y: Int[Array, \"n c\"], batch_size: int = 64\n",
    ") -> Iterator[tuple[Float[Array, \"batch h w ch\"], Int[Array, \"batch c\"]]]:\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        yield X[i : i + batch_size], y[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = globals().get(\"dataset\") or load_dataset(\"cifar10\")\n",
    "\n",
    "train_size = len(dataset[\"train\"]) // 10\n",
    "val_size = len(dataset[\"test\"]) // 10\n",
    "\n",
    "X_train = jnp.array([dataset[\"train\"][i][\"img\"] for i in range(train_size)])\n",
    "y_train = jnp.array([dataset[\"train\"][i][\"label\"] for i in range(train_size)])\n",
    "y_train = jax.nn.one_hot(y_train, num_classes=10).astype(jnp.int32)\n",
    "\n",
    "X_val = jnp.array([dataset[\"test\"][i][\"img\"] for i in range(val_size)])\n",
    "y_val = jnp.array([dataset[\"test\"][i][\"label\"] for i in range(val_size)])\n",
    "y_val = jax.nn.one_hot(y_val, num_classes=10).astype(jnp.int32)\n",
    "\n",
    "idx2cls = {i: cls for i, cls in enumerate(dataset[\"train\"].features[\"label\"].names)}\n",
    "cls2idx = {cls: i for i, cls in idx2cls.items()}\n",
    "num_classes = len(idx2cls)\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_train = X_train.astype(jnp.float32)\n",
    "X_val = X_val / 255.0\n",
    "X_val = X_val.astype(jnp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = VITConfig(rngs=nnx.Rngs(0))\n",
    "model = VITClassifier(config=config, num_classes=num_classes)\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate=3e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(\n",
    "    model: VITClassifier, X: Float[Array, \"batch h w ch\"], y: Int[Array, \"batch c\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    logits = model(X)\n",
    "    loss = optax.softmax_cross_entropy(logits, y).mean()\n",
    "    return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(\n",
    "    logits: Float[Array, \"batch c\"], y: Int[Array, \"batch c\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    return (logits.argmax(axis=-1) == y.argmax(axis=-1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typed\n",
    "@nnx.jit\n",
    "def train_step(\n",
    "    model: VITClassifier,\n",
    "    X: Float[Array, \"batch h w ch\"],\n",
    "    y: Int[Array, \"batch c\"],\n",
    "    optimizer: nnx.Optimizer,\n",
    "    metrics: nnx.Metric,\n",
    ") -> None:\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(model, X, y)\n",
    "    acc = accuracy(logits, y)\n",
    "    metrics.update(loss=loss, accuracy=acc)\n",
    "    optimizer.update(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typed\n",
    "@nnx.jit\n",
    "def val_step(\n",
    "    model: VITClassifier,\n",
    "    X: Float[Array, \"batch h w ch\"],\n",
    "    y: Int[Array, \"batch c\"],\n",
    "    metrics: nnx.Metric,\n",
    ") -> None:\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(model, X, y)\n",
    "    acc = accuracy(logits, y)\n",
    "    metrics.update(loss=loss, accuracy=acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "metrics = nnx.metrics.MultiMetric(\n",
    "    accuracy=nnx.metrics.Average(\"accuracy\"), loss=nnx.metrics.Average(\"loss\")\n",
    ")\n",
    "pbar = tqdm(range(EPOCHS))\n",
    "for epoch in pbar:\n",
    "    metrics.reset()\n",
    "    for X, y in dataloader(X_train, y_train, batch_size=64):\n",
    "        # with jax.checking_leaks():\n",
    "        train_step(model, X, y, optimizer, metrics)\n",
    "    m = metrics.compute()\n",
    "    train_loss = float(m[\"loss\"])\n",
    "    train_acc = float(m[\"accuracy\"])\n",
    "\n",
    "    metrics.reset()\n",
    "    for X, y in dataloader(X_val, y_val, batch_size=64):\n",
    "        val_step(model, X, y, metrics)\n",
    "    m = metrics.compute()\n",
    "\n",
    "    val_loss = float(m[\"loss\"])\n",
    "    val_acc = float(m[\"accuracy\"])\n",
    "\n",
    "    pbar.set_description(\n",
    "        f\"Epoch {epoch} train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
