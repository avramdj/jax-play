{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "from datasets import load_dataset\n",
    "from jaxtyping import Array, Float, Int, jaxtyped\n",
    "from beartype import beartype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lovely_jax\n",
    "lovely_jax.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typed(fn):\n",
    "    return jaxtyped(fn, typechecker=beartype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VITConfig:\n",
    "    in_feature_shape = (32, 32, 3)\n",
    "    out_features = 10\n",
    "    patch_size = 4\n",
    "    num_layers = 8\n",
    "    num_heads = 8\n",
    "    embed_dim = 256\n",
    "    rngs: nnx.Rngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nnx.Module):\n",
    "    def __init__(self, module: nnx.Module):\n",
    "        self.norm = nnx.LayerNorm(\n",
    "            num_features=config.embed_dim,\n",
    "            rngs=config.rngs,\n",
    "        )\n",
    "        self.module = module\n",
    "    \n",
    "    @typed\n",
    "    @nnx.jit()\n",
    "    def __call__(self, x: Float[Array, \"batch ...\"]) -> Float[Array, \"batch ...\"]:\n",
    "        x = self.norm(x)\n",
    "        return x + self.module(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchify(nnx.Module):\n",
    "    def __init__(self, *, config: VITConfig):\n",
    "        self.config = config\n",
    "        self.conv = nnx.Conv(\n",
    "            in_features=config.in_feature_shape[2],\n",
    "            out_features=config.embed_dim,\n",
    "            kernel_size=(config.patch_size, config.patch_size),\n",
    "            strides=(config.patch_size, config.patch_size),\n",
    "            rngs=config.rngs,\n",
    "        )\n",
    "    \n",
    "    @typed\n",
    "    @nnx.jit()\n",
    "    def __call__(self, x: Float[Array, \"batch h w c\"]) -> Float[Array, \"batch patches emb\"]:\n",
    "        x = self.conv(x)\n",
    "        x = x.reshape(x.shape[0], -1, self.config.embed_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typed\n",
    "@nnx.jit()\n",
    "def apply_rope(\n",
    "    q: Float[Array, \"batch n d\"],\n",
    "    k: Float[Array, \"batch n d\"],\n",
    ") -> tuple[Float[Array, \"batch n d\"], Float[Array, \"batch n d\"]]:\n",
    "    return q, k  #TODO: implement rope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnBlock(nnx.Module):\n",
    "    def __init__(self, *, config: VITConfig):\n",
    "        self.config = config\n",
    "        self.qkv = nnx.Linear(\n",
    "            in_features=config.embed_dim,\n",
    "            out_features=config.embed_dim * 3,\n",
    "            rngs=config.rngs,\n",
    "        )\n",
    "\n",
    "    @typed\n",
    "    @nnx.jit()\n",
    "    def __call__(self, x: Float[Array, \"batch patches emb\"]) -> Float[Array, \"batch patches emb\"]:\n",
    "        q, k, v = self.qkv(x).split(3, axis=-1)\n",
    "        q, k = apply_rope(q, k)\n",
    "        a = nnx.dot_product_attention(q, k, v)\n",
    "        a = a.reshape(a.shape[0], -1, self.config.embed_dim)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nnx.Module):\n",
    "    def __init__(self, *, config: VITConfig):\n",
    "        self.config = config\n",
    "        self.norm = nnx.LayerNorm(\n",
    "            num_features=config.embed_dim,\n",
    "            rngs=config.rngs,\n",
    "        )\n",
    "        self.linear1 = nnx.Linear(\n",
    "            in_features=config.embed_dim,\n",
    "            out_features=config.embed_dim * 4,\n",
    "            rngs=config.rngs,\n",
    "        )\n",
    "        self.linear2 = nnx.Linear(\n",
    "            in_features=config.embed_dim * 4,\n",
    "            out_features=config.embed_dim,\n",
    "            rngs=config.rngs,\n",
    "        )\n",
    "\n",
    "    @typed\n",
    "    @nnx.jit()\n",
    "    def __call__(self, x: Float[Array, \"batch patches emb\"]) -> Float[Array, \"batch patches emb\"]:\n",
    "        x = self.norm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nnx.Module):\n",
    "    def __init__(self, *, config: VITConfig):\n",
    "        self.config = config\n",
    "        self.mha = Residual(AttnBlock(config=config))\n",
    "        self.mlp = Residual(MLP(config=config))\n",
    "\n",
    "    @typed\n",
    "    @nnx.jit()\n",
    "    def __call__(self, x: Float[Array, \"batch patches emb\"]) -> Float[Array, \"batch patches emb\"]:\n",
    "        x = self.mha(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nnx.Module):\n",
    "    def __init__(self, *, config: VITConfig):\n",
    "        self.config = config\n",
    "        self.layers = nnx.Sequential(\n",
    "            *[EncoderBlock(config=config) for _ in range(config.num_layers)]\n",
    "        )\n",
    "\n",
    "    @typed\n",
    "    @nnx.jit()\n",
    "    def __call__(self, x: Float[Array, \"batch patches emb\"]) -> Float[Array, \"batch patches emb\"]:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nnx.Module):\n",
    "    def __init__(self, *, config: VITConfig):\n",
    "        self.config = config\n",
    "        self.patchify = Patchify(config=config)\n",
    "        self.encoder = Encoder(config=config)\n",
    "    \n",
    "    @typed\n",
    "    @nnx.jit()\n",
    "    def __call__(self, x: Float[Array, \"batch h w c\"]) -> Float[Array, \"batch patches emb\"]:\n",
    "        x = self.patchify(x)\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(\n",
    "    X: Float[Array, \"n h w c\"], y: Int[Array, \"n c\"], batch_size: int = 64\n",
    ") -> Iterator[tuple[Float[Array, \"batch h w c\"], Int[Array, \"batch c\"]]]:\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        yield X[i : i + batch_size], y[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cifar10\")\n",
    "\n",
    "X_images = jnp.array([dataset[\"train\"][i][\"img\"] for i in range(1024)])\n",
    "X_labels = jnp.array([dataset[\"train\"][i][\"label\"] for i in range(1024)])\n",
    "\n",
    "X_images = X_images / 255.0\n",
    "X_images = X_images.astype(jnp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = VITConfig(rngs=nnx.Rngs(0))\n",
    "vit = VIT(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
